{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a5b3e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization,Input\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers,initializers\n",
    "from keras.layers import GaussianNoise\n",
    "import keras.backend as Kb\n",
    "import keras.losses\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "# Library for Gaussian process\n",
    "# import GPy\n",
    "##Library for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib;matplotlib.rcParams['figure.figsize'] = (8,6)\n",
    "import pylab \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cbe0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Functions for calculation of MSE and MAE\n",
    "# def mse(y_pred,y_true):\n",
    "#     mse = np.mean((y_pred-y_true)**2)\n",
    "#     return mse\n",
    "\n",
    "# def mae(y_pred,y_true):\n",
    "#     mae = np.mean(np.absolute(y_pred-y_true))\n",
    "#     return mae\n",
    "\n",
    "# s_train, s_test, encoder_train, encoder_test    , y_train, y_test= train_test_split(s, phi_reduce, y, \n",
    "#                                                                                 test_size=0.3333)\n",
    "# N_train = s_train.shape[0]\n",
    "# N_test = s_test.shape[0]\n",
    "\n",
    "\n",
    "# # DeepKriging model for continuous data\n",
    "# model = Sequential()\n",
    "# # model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "# model.add(Dense(100, input_dim = encoder_train.shape[1],  \n",
    "#             kernel_initializer=initializers.RandomNormal(stddev=0.01), activation='relu'))\n",
    "# #     model.add(Dense(100, input_dim = encoder_train.shape[1],  kernel_initializer='he_uniform', activation='relu'))\n",
    "# # model.add(Dropout(rate=0.5))\n",
    "# # model.add(BatchNormalization())\n",
    "# #     model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "#             bias_regularizer=regularizers.l2(1e-4),\n",
    "#             activity_regularizer=regularizers.l2(1e-5),activation='relu'))\n",
    "# #     model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# #     model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# # model.add(Dense(100, activation='relu'))\n",
    "# #model.add(Dropout(rate=0.5))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# #model.add(BatchNormalization())\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "# # model.compile(optimizer=optimizer, loss='mse', metrics=['mae','mse'])\n",
    "\n",
    "# # result = model.fit(encoder_train, y_train, \n",
    "# #                    validation_data=(encoder_test,y_test), epochs = 100, batch_size = 64, verbose = 2)\n",
    "\n",
    "# # callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "# #              ModelCheckpoint(filepath='comp-2a-1.h5', monitor='val_loss', save_best_only=True)]\n",
    "# # result = model.fit(encoder_train, y_train, callbacks=callbacks, \n",
    "# #                    validation_data=(encoder_test,y_test), epochs = 100, batch_size = 64, verbose = 2)\n",
    "# # model = keras.models.load_model('comp-2a-1.h5')\n",
    "# # y_pred = model.predict(encoder_test)\n",
    "\n",
    "\n",
    "# # # Mean Squared Error\n",
    "# # mse_var1.append(mse(y_pred[:,0], y_test[:,0]))\n",
    "# # print(mse_var1)\n",
    "\n",
    "# # end_time = time.time()\n",
    "# # print(\"%s seconds\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "578cd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(model, X_train, y_train, X_valid, y_valid, data_type):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    NB_START_EPOCHS = 20\n",
    "    BATCH_SIZE = 512\n",
    "    if data_type == 'continuous':\n",
    "        model.compile(optimizer='adam'\n",
    "                      , loss='mse'\n",
    "                      , metrics=['mse','mae'])\n",
    "    if data_type == 'discrete':\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=0)\n",
    "    return history\n",
    "\n",
    "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
    "    '''\n",
    "    Function to test the model on new data after training it\n",
    "    on the full training data with the optimal number of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        model : trained model\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_test : test features\n",
    "        y_test : test target\n",
    "        epochs : optimal number of epochs\n",
    "    Output:\n",
    "        test accuracy and test loss\n",
    "    '''\n",
    "    BATCH_SIZE = 512\n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return results\n",
    "    \n",
    "def optimal_epoch(model_hist):\n",
    "    '''\n",
    "    Function to return the epoch number where the validation loss is\n",
    "    at its minimum\n",
    "    \n",
    "    Parameters:\n",
    "        model_hist : training history of model\n",
    "    Output:\n",
    "        epoch number with minimum validation loss\n",
    "    '''\n",
    "    min_epoch = np.argmin(model_hist.history['val_loss']) + 1\n",
    "    return min_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a0f0a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_func(y_pred,y_true):\n",
    "    mse = np.mean((y_pred-y_true)**2)\n",
    "    return mse\n",
    "\n",
    "def mae_func(y_pred,y_true):\n",
    "    mae = np.mean(np.absolute(y_pred-y_true))\n",
    "    return mae\n",
    "# define and fit the model\n",
    "def fit_model(X_train, y_train,X_test,y_test):\n",
    "    # DeepKriging model for continuous data\n",
    "    ensemble_model = Sequential()\n",
    "\n",
    "    ensemble_model.add(base_model1)\n",
    "#     ensemble_model.add(Dense(50, activation = \"relu\"))\n",
    "    ensemble_model.add(Dense(50, activation = \"relu\"))\n",
    "    ensemble_model.add(Dense(1, activation='linear'))\n",
    "    ensemble_model.layers[-3].trainable = False\n",
    "    optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "    ensemble_model.compile(optimizer=optimizer, loss='mse', metrics=['mse','mae'])\n",
    "    ensemble_model.fit(X_train, y_train,#validation_data=(X_test,y_test), \n",
    "                       epochs = 50, batch_size = 256, verbose = 0)\n",
    "    return ensemble_model\n",
    "\n",
    "# fit an ensemble of models\n",
    "def fit_ensemble(n_members, X_train, Y_train):\n",
    "    ensemble = list()\n",
    "    for i in tqdm(range(n_members)):\n",
    "#         x_train, x_test,y_train,y_test = train_test_split(X_train,Y_train,test_size = 0.1)\n",
    "        # define and fit the model on the training set\n",
    "        model = fit_model(X_train, Y_train,X_train,Y_train)\n",
    "        # evaluate model on the test set\n",
    "#         yhat = model.predict(x_test, verbose=0)\n",
    "#         mae1 = mae(yhat, y_test)\n",
    "#         print('>%d, MAE: %.3f' % (i+1, mae1))\n",
    "        # store the model\n",
    "        ensemble.append(model)\n",
    "    return ensemble\n",
    "def y_list_uni(yhat,i):\n",
    "    the_list = list()\n",
    "    for j in range(len(yhat)):\n",
    "        the_list.append(yhat[j][0][i])\n",
    "    return the_list\n",
    "# make predictions with the ensemble and calculate a prediction interval\n",
    "def variance(data):\n",
    "    # Number of observations\n",
    "    n = len(data)\n",
    "    # Mean of the data\n",
    "    mean = sum(data) / n\n",
    "    # Square deviations\n",
    "    deviations = [(x - mean) ** 2 for x in data]\n",
    "    # Variance\n",
    "    variance = sum(deviations) / (n-1)\n",
    "    return variance\n",
    "def predict_with_pi(ensemble, X):\n",
    "    mean_vec1 = list()\n",
    "    var_vec1 = list()\n",
    "    # make predictions\n",
    "    yhat = [model.predict(X, verbose=0) for model in ensemble]\n",
    "#     print(yhat)\n",
    "    for data in tqdm(range(X.shape[0])):\n",
    "        yhat1 = np.asarray(np.asarray(yhat)[:,data,:])\n",
    "#         var1_pred = yhat1[:,0]\n",
    "#         var2_pred = yhat1[:,1]\n",
    "        mean1 = yhat1.mean()\n",
    "        var1 = variance(yhat1)\n",
    "        mean_vec1.append(mean1)\n",
    "        var_vec1.append(var1)\n",
    "    return mean_vec1, var_vec1\n",
    "\n",
    "def calc_distance(s1,s2):\n",
    "    return(np.sqrt((s1[0] - s2[0])**2 + (s1[1] - s2[1])**2))\n",
    "\n",
    "def get_nearest_data(s_train,s_test,r,k):\n",
    "    dist_mat = np.zeros((len(s_test),len(s_train)))\n",
    "    nearest_var = np.zeros((len(s_test)))\n",
    "    print(f'running get_nearest_data function ...')\n",
    "    for i in tqdm(range(len(s_train))):\n",
    "        for j in range(len(s_test)):\n",
    "            dist_mat[j][i] = calc_distance(s_train[i],s_test[j])\n",
    "    \n",
    "    for i in tqdm(range(len(s_test))):\n",
    "        a = dist_mat[i]\n",
    "        b = np.argpartition(a,k)[:k]\n",
    "        y_val = []\n",
    "        for index in b:\n",
    "            y_val.append(r[index])\n",
    "        nearest_var[i] = (np.mean(y_val))\n",
    "    print(f'Run finished ...')\n",
    "    return nearest_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "76f061d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_folds = 5\n",
    "# kfold = KFold(n_splits=num_folds, shuffle=True, random_state = 123)\n",
    "# fold_no = 1\n",
    "# inputs = phi_reduce\n",
    "# targets = y\n",
    "# mse_per_fold = []\n",
    "# mae_per_fold = []\n",
    "# for train_idx, test_idx in kfold.split(inputs, targets):\n",
    "#     print('------------------------------------------------------------------------')\n",
    "#     print(f'Training for fold {fold_no} ...')\n",
    "#     history = deep_model(model, inputs[train_idx], targets[train_idx]\\\n",
    "#                               , inputs[test_idx], targets[test_idx],'continuous')\n",
    "\n",
    "#     model_optim = 20#optimal_epoch(history)\n",
    "#     result = test_model(model, inputs[train_idx], targets[train_idx], inputs[test_idx]\\\n",
    "#                         , targets[test_idx], model_optim)\n",
    "#     scores = result\n",
    "#     print(f'The performance of DeepKriging: MSE = {scores[1]}, MAE = {scores[2]}')\n",
    "#     fold_no = fold_no + 1\n",
    "#     mse_per_fold.append(scores[1])\n",
    "# #     mse_per_fold_base.append(scores_base[1])\n",
    "#     mae_per_fold.append(scores[2])\n",
    "# #     mae_per_fold_base.append(scores_base[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5f0b4c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data_no in range(5):\n",
    "    df_loc = pd.read_csv(\"Sub-competition_2a/Training data/LOC_Train_\"+str(data_no+1)+\".csv\",sep = \",\", \n",
    "                         header = None, names=['x', 'y'])\n",
    "    df_val = pd.read_csv(\"Sub-competition_2a/Training data/Z_Train_\"+str(data_no+1)+\".csv\",sep = \",\", \n",
    "                         header = None, names=['data'])\n",
    "    df_loc,_,df_val,_ = train_test_split(df_loc, df_val,  \n",
    "                                                test_size=0.3)\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training on dataset 2a_{data_no+1}...')\n",
    "    N = len(df_loc)\n",
    "    s = np.vstack((df_loc[\"x\"],df_loc[\"y\"])).T\n",
    "    y = df_val.iloc[:,0].values\n",
    "    ### Basis functions\n",
    "\n",
    "    num_basis = [10**2,19**2,37**2]\n",
    "    knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
    "    ##Wendland kernel\n",
    "    K = 0\n",
    "    phi = np.zeros((N, sum(num_basis)))\n",
    "    for res in range(len(num_basis)):\n",
    "        theta = 1/np.sqrt(num_basis[res])*2.5\n",
    "        knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
    "        knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
    "        for i in range(num_basis[res]):\n",
    "            d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
    "            for j in range(len(d)):\n",
    "                if d[j] >= 0 and d[j] <= 1:\n",
    "                    phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "                else:\n",
    "                    phi[j,i + K] = 0\n",
    "        K = K + num_basis[res]\n",
    "        ## Romove the all-zero columns\n",
    "    idx_zero = np.array([], dtype=int)\n",
    "    for i in range(phi.shape[1]):\n",
    "        if sum(phi[:,i]!=0)==0:\n",
    "            idx_zero = np.append(idx_zero,int(i))\n",
    "\n",
    "    phi_reduce = np.delete(phi,idx_zero,1)\n",
    "    \n",
    "    num_folds = 2\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state = 123)\n",
    "    inputs = phi_reduce\n",
    "    targets = y\n",
    "    count = 0\n",
    "    mse_vec = np.empty([num_folds,1])\n",
    "    mae_vec = np.empty([num_folds,1])\n",
    "    mpiw_vec = np.empty([num_folds,1])\n",
    "    picp_vec = np.empty([num_folds,1])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for train_idx, test_idx in kfold.split(inputs, targets):\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f'Cross-validation iteration {count} ...')\n",
    "        input_dim = Input(shape = (phi_reduce.shape[1], ))\n",
    "        layer1 = Dense(100, kernel_initializer='he_uniform', activation = 'relu')(input_dim)\n",
    "        layer2 = Dense(100, activation = 'relu')(layer1)\n",
    "        layer3 = Dense(100, activation = 'relu')(layer2)\n",
    "        layer4 = Dense(100, activation = 'relu')(layer3)\n",
    "        layer5 = Dense(50, activation = 'relu')(layer4)\n",
    "        layer6 = Dense(50, activation = 'relu')(layer5)\n",
    "        final_layer = Dense(1, activation = 'linear')(layer5)\n",
    "\n",
    "\n",
    "        s_train_ensemble, s_train_mse, X_train_ensemble, X_train_mse, \\\n",
    "        y_train_ensemble, y_train_mse = train_test_split(s[train_idx], \n",
    "                                                        inputs[train_idx], \n",
    "                                                        targets[train_idx], \n",
    "                                                test_size=0.25)\n",
    "\n",
    "        s_train_ensemble1, s_train_ensemble2, X_train_ensemble1, \\\n",
    "        X_train_ensemble2, y_train_ensemble1, y_train_ensemble2 = train_test_split(s_train_ensemble, \n",
    "                                                                                  X_train_ensemble, \n",
    "                                                                                  y_train_ensemble, \n",
    "                                                                                  test_size=0.3)\n",
    "\n",
    "        base_model = Model(inputs = input_dim, outputs = final_layer)\n",
    "        optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "        # Compile the Model\n",
    "        base_model.compile(optimizer = optimizer, loss = 'mse')\n",
    "        base_model.fit(X_train_ensemble1, y_train_ensemble1,validation_split = 0.1, epochs = 100,\n",
    "                        batch_size = 512,verbose = 0)\n",
    "\n",
    "        base_model1 = Model(inputs = input_dim, outputs = layer5)\n",
    "\n",
    "        n_members = 10\n",
    "        ensemble = fit_ensemble(n_members, X_train_ensemble, y_train_ensemble)\n",
    "\n",
    "        # train data mean and variance vectors\n",
    "        mean_vec1, var_vec1 = predict_with_pi(ensemble, X_train_mse)\n",
    "\n",
    "        mean_vec1 = np.asarray(mean_vec1)\n",
    "        var_vec1 = np.asarray(var_vec1)\n",
    "        # random error calculation on training data\n",
    "        r1 = ((y_train_mse - mean_vec1)**2).reshape(y_train_mse.shape[0],1) - var_vec1\n",
    "        for i in range(len(r1)):\n",
    "            if r1[i] < 0: r1[i] = 0.0\n",
    "\n",
    "        # random error calculation on test data with neighbourhood approach\n",
    "\n",
    "        s_test = s[test_idx]\n",
    "        y_test = targets[test_idx]\n",
    "        r1_pred = get_nearest_data(s_train_mse,s_test,r1,50).reshape(s_test.shape[0],1)\n",
    "\n",
    "        # mean and variance vector for prediction data\n",
    "        mean_vec1, var_vec1 = predict_with_pi(ensemble, inputs[test_idx])\n",
    "\n",
    "\n",
    "        end_time = time.time()\n",
    "#         print(\"%s seconds\", end_time - start_time)\n",
    "        var_vec1 = np.asarray(var_vec1)\n",
    "        mean_vec1 = np.asarray(mean_vec1).reshape(var_vec1.shape[0],1)\n",
    "\n",
    "        # e = np.asarray(e)\n",
    "        lower_bound1 = mean_vec1 - 1.96*np.sqrt(var_vec1 + r1_pred)\n",
    "        upper_bound1 = mean_vec1 + 1.96*np.sqrt(var_vec1 + r1_pred)\n",
    "\n",
    "        count_var0 = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if ((y_test[i] > lower_bound1[i]) and (y_test[i] < upper_bound1[i])): count_var0 +=1\n",
    "\n",
    "\n",
    "        mse = mse_func(mean_vec1,y_test.reshape(var_vec1.shape[0],1))  \n",
    "        mae = mae_func(mean_vec1,y_test.reshape(var_vec1.shape[0],1))\n",
    "        picp= count_var0/len(y_test)\n",
    "        mpiw = np.mean(upper_bound1 - lower_bound1)\n",
    "        mse_vec[(count-1)] = mse\n",
    "        mae_vec[(count-1)] = mae\n",
    "        picp_vec[(count-1)] = picp\n",
    "        mpiw_vec[(count-1)] = mpiw\n",
    "        print(f'Performance on cross-validation {count}: MSE = {mse}, MAE = {mae}, picp = {picp}, mpiw = {mpiw}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    mse = np.mean(mse_vec)\n",
    "    mae = np.mean(mae_vec)\n",
    "    picp = np.mean(picp_vec)\n",
    "    mpiw = np.mean(mpiw_vec)\n",
    "    time_t = (start_time - end_time)/num_folds\n",
    "    print(f'Average performance on 2a-{data_no+1}: MSE = {mse}, MAE = {mae}, picp = {picp}, mpiw = {mpiw}, time = {-time_t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09275c81",
   "metadata": {},
   "source": [
    "# Application on Precipitation data over USA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5d408d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"precip_data_real.csv\",sep = \",\")\n",
    "# df,_= train_test_split(df,\n",
    "#                                             test_size=0.5)\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training on precipitation dataset ...')\n",
    "N = len(df)\n",
    "s = np.vstack((df[\"x\"],df[\"y\"])).T\n",
    "y = df.iloc[:,2].values\n",
    "### Basis functions\n",
    "\n",
    "num_basis = [10**2,19**2,37**2]\n",
    "knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
    "##Wendland kernel\n",
    "K = 0\n",
    "phi = np.zeros((N, sum(num_basis)))\n",
    "for res in range(len(num_basis)):\n",
    "    theta = 1/np.sqrt(num_basis[res])*2.5\n",
    "    knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
    "    knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
    "    for i in range(num_basis[res]):\n",
    "        d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
    "        for j in range(len(d)):\n",
    "            if d[j] >= 0 and d[j] <= 1:\n",
    "                phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "            else:\n",
    "                phi[j,i + K] = 0\n",
    "    K = K + num_basis[res]\n",
    "    ## Romove the all-zero columns\n",
    "idx_zero = np.array([], dtype=int)\n",
    "for i in range(phi.shape[1]):\n",
    "    if sum(phi[:,i]!=0)==0:\n",
    "        idx_zero = np.append(idx_zero,int(i))\n",
    "\n",
    "phi_reduce = np.delete(phi,idx_zero,1)\n",
    "\n",
    "num_folds = 3\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state = 123)\n",
    "inputs = phi_reduce\n",
    "targets = y\n",
    "count = 0\n",
    "mse_vec = np.empty([num_folds,1])\n",
    "mae_vec = np.empty([num_folds,1])\n",
    "mpiw_vec = np.empty([num_folds,1])\n",
    "picp_vec = np.empty([num_folds,1])\n",
    "\n",
    "start_time = time.time()\n",
    "for train_idx, test_idx in kfold.split(inputs, targets):\n",
    "    count += 1\n",
    "#     df_train = pd.DataFrame(s[train_idx], columns=['lon','lat'])\n",
    "#     df_train[\"data\"] = y[train_idx]\n",
    "#     df_test = pd.DataFrame(s[test_idx], columns=['lon','lat'])\n",
    "#     df_test[\"data\"] = y[test_idx]\n",
    "#     mypath = \"real_data/cross_val_\"+str(count)+\"/\"\n",
    "#     if not os.path.isdir(mypath):\n",
    "#         os.makedirs(mypath)\n",
    "#     df_train.to_csv(mypath+\"train.csv\",index = False)\n",
    "#     df_test.to_csv(mypath+\"test.csv\",index = False)\n",
    "    print(f'Cross-validation iteration {count} ...')\n",
    "    input_dim = Input(shape = (phi_reduce.shape[1], ))\n",
    "    layer1 = Dense(100, kernel_initializer='he_uniform', activation = 'relu')(input_dim)\n",
    "    layer2 = Dense(100, activation = 'relu')(layer1)\n",
    "    layer3 = Dense(100, activation = 'relu')(layer2)\n",
    "    layer4 = Dense(100, activation = 'relu')(layer3)\n",
    "    layer5 = Dense(50, activation = 'relu')(layer4)\n",
    "    layer6 = Dense(50, activation = 'relu')(layer5)\n",
    "    final_layer = Dense(1, activation = 'linear')(layer5)\n",
    "\n",
    "\n",
    "    s_train_ensemble, s_train_mse, X_train_ensemble, X_train_mse, \\\n",
    "    y_train_ensemble, y_train_mse = train_test_split(s[train_idx], \n",
    "                                                    inputs[train_idx], \n",
    "                                                    targets[train_idx], \n",
    "                                            test_size=0.25)\n",
    "\n",
    "    s_train_ensemble1, s_train_ensemble2, X_train_ensemble1, \\\n",
    "    X_train_ensemble2, y_train_ensemble1, y_train_ensemble2 = train_test_split(s_train_ensemble, \n",
    "                                                                              X_train_ensemble, \n",
    "                                                                              y_train_ensemble, \n",
    "                                                                              test_size=0.3)\n",
    "\n",
    "    base_model = Model(inputs = input_dim, outputs = final_layer)\n",
    "    optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "    # Compile the Model\n",
    "    base_model.compile(optimizer = optimizer, loss = 'mse')\n",
    "    base_model.fit(X_train_ensemble1, y_train_ensemble1,validation_split = 0.1, epochs = 100,\n",
    "                    batch_size = 512,verbose = 0)\n",
    "\n",
    "    base_model1 = Model(inputs = input_dim, outputs = layer5)\n",
    "\n",
    "    n_members = 10\n",
    "    ensemble = fit_ensemble(n_members, X_train_ensemble, y_train_ensemble)\n",
    "\n",
    "    # train data mean and variance vectors\n",
    "    mean_vec1, var_vec1 = predict_with_pi(ensemble, X_train_mse)\n",
    "\n",
    "    mean_vec1 = np.asarray(mean_vec1)\n",
    "    var_vec1 = np.asarray(var_vec1)\n",
    "    # random error calculation on training data\n",
    "    r1 = ((y_train_mse - mean_vec1)**2).reshape(y_train_mse.shape[0],1) - var_vec1\n",
    "    for i in range(len(r1)):\n",
    "        if r1[i] < 0: r1[i] = 0.0\n",
    "\n",
    "    # random error calculation on test data with neighbourhood approach\n",
    "\n",
    "    s_test = s[test_idx]\n",
    "    y_test = targets[test_idx]\n",
    "    r1_pred = get_nearest_data(s_train_mse,s_test,r1,50).reshape(s_test.shape[0],1)\n",
    "\n",
    "    # mean and variance vector for prediction data\n",
    "    mean_vec1, var_vec1 = predict_with_pi(ensemble, inputs[test_idx])\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "#         print(\"%s seconds\", end_time - start_time)\n",
    "    var_vec1 = np.asarray(var_vec1)\n",
    "    mean_vec1 = np.asarray(mean_vec1).reshape(var_vec1.shape[0],1)\n",
    "\n",
    "    # e = np.asarray(e)\n",
    "    lower_bound1 = mean_vec1 - 1.96*np.sqrt(var_vec1 + r1_pred)\n",
    "    upper_bound1 = mean_vec1 + 1.96*np.sqrt(var_vec1 + r1_pred)\n",
    "\n",
    "    count_var0 = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if ((y_test[i] > lower_bound1[i]) and (y_test[i] < upper_bound1[i])): count_var0 +=1\n",
    "\n",
    "\n",
    "    mse = mse_func(mean_vec1,y_test.reshape(19918,1))  \n",
    "    mae = mae_func(mean_vec1,y_test.reshape(19918,1))\n",
    "    picp= count_var0/len(y_test)\n",
    "    mpiw = np.mean(upper_bound1 - lower_bound1)\n",
    "    mse_vec[(count-1)] = mse\n",
    "    mae_vec[(count-1)] = mae\n",
    "    picp_vec[(count-1)] = picp\n",
    "    mpiw_vec[(count-1)] = mpiw\n",
    "    print(f'Performance on cross-validation {count}: MSE = {mse}, MAE = {mae}, picp = {picp}, mpiw = {mpiw}')\n",
    "\n",
    "end_time = time.time()\n",
    "mse = np.mean(mse_vec)\n",
    "mae = np.mean(mae_vec)\n",
    "picp = np.mean(picp_vec)\n",
    "mpiw = np.mean(mpiw_vec)\n",
    "time_t = (start_time - end_time)/num_folds\n",
    "print(f'Average performance on precipitation data: MSE = {mse}, MAE = {mae}, picp = {picp}, mpiw = {mpiw}, time = {-time_t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51abbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
